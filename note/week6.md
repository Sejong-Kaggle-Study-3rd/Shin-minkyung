# 의사결정나무(Decision Tree)

# 1. 정의

의사결정나무: 학습 데이터에 **내재되어 있는 패턴**으로 새로운 데이터를 예측, 분류

*질문을 던져서 대상을 좁혀나가는 스무고개와 비슷한 개념*

분리기준(질문) + 정지규칙

장점

1. 이해하기 쉽고 적용하기 쉬움

    나무구조: If-then 규칙

2. **의사결정과정에 대한 해석 가능**

    ex. 야구 경기의 취소 사례의 이유를 설명 가능 

3. 중요한 변수 선택에 유용

    중요한 변수 = 상단의 설명변수 : 위에 있는 분리기준이 더 중요

    → 중요한 질문 먼저 한다.

4. 데이터의 통계적 가정이 불필요

    ex. LDA: 데이터의 정규성을 가정 

단점

1. 많은 데이터 필요
2. 시간 많이 소요: Tree Building
3. 데이터의 변화에 민감

    학습데이터와 테스트데이터의 도메인이 유사해야함

4. 선형구조형 데이터 예측 시 더 복잡할 수 있음

    선형회귀로 결정 경계를 만드는 것이 더 나을 수 있음

    의사결정나무: 수직으로 계속 나눔 

---

# 2. 의사결정나무 과정

## 나무 모델 생성 → 과적합 문제 해결 → 검증 → 해석 및 예측

### 1. 나무 모델 생성

- **재귀적 분할**

    데이터 순도가 균일해지도록 재귀적 분할(끝 노드에 비슷한 클래스, 수치가 있도록 2개 이상의 부분집합으로 분할)

    1. CART
    2. C4.5, C5.0
    3. CHAID

    - **분할기준: 불순도**
        1. 지니 지수(Gini index)
        2. 엔트로피 지수, 정보이익, 정보이익률 (Entropy index, Information Gain, Information gain ratio)
        3. 카이제곱 통계량 (Chi-Square Statistic)

### 2. 과적합 문제 해결

1. **성장 멈추기 (Stop Condition)**
    1. 나무 모델의 깊이 파리미터로 설정
    2. 특정 조건에서 트리 성장 중단
    3. 노드 내 최소 관측치 수

        노드 내 최소 관측치 수 = 2로 설정

        → 2 이하가 되면 트리 성장 중단

    4. 불순도 최소 감소량 

        CHAID에서 사용

        가지치기 사용하지 않고 종료

2. **가지치기 (Purning)** : 다 해보고 결정하자

완전 모형 생성 후 가지치기

나무 모델 생성 후 필요 없는 가지 제거 (merge)

성장 멈추기 보다 성능 우수 

가지치기 비용함수를 최소로 하는 분기 찾음

### 3. 검증 및 해석

회귀 성능평가: RSME(Root Mean Square Error)

### 4. 예측

분류: 끝 노드에서 가장 많은 class로 voting, 경향성(확률표현가능)

회귀: 끝 노드의 평균값

---

## 1. 분류 나무(Classification Tree)

### 1. 이진분할 = 데이터 공간에 대한 분할

**CART(Gini index)**

나무 생성(학습데이터) → 가지치기(검증용데이터

Gini index (불확실성)은 낮아지는게 좋다

*Gini index가 작아지는 설명변수를 택*

### 2. 다중분할

1. **C4.5**
    - 나무 생성→ 가지치기: only 학습데이터
    1. **Entropy index** (불확실성)
    2. **Information Gain**

        정보의 가치가 높아야 좋다

        IG = E(before) - E(After)

    3. **Information gain ratio**
        - 가지 ↑ → information gain

            다중분할일수록 IG ↑ → Scale이 맞춰지지 않음

            ⇒ 정규화 과정이 필요 

        - IV(Intrinsic Value): 정보이득율을 정규화

            ![%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%82%E1%85%A1%E1%84%86%E1%85%AE(Decision%20Tree)%200e4992deeebb465398bc323ade36235f/Untitled.png](%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%82%E1%85%A1%E1%84%86%E1%85%AE(Decision%20Tree)%200e4992deeebb465398bc323ade36235f/Untitled.png)

            ![%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%82%E1%85%A1%E1%84%86%E1%85%AE(Decision%20Tree)%200e4992deeebb465398bc323ade36235f/Untitled%201.png](%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%82%E1%85%A1%E1%84%86%E1%85%AE(Decision%20Tree)%200e4992deeebb465398bc323ade36235f/Untitled%201.png)

2. **CHAID(카이제곱통계량)**

## 2. 회귀 나무(Regression Tree)

**CART(F통계량, 분산감소량)**

실제값과 예측값의 평균 차이가 작아지도록 

---

# 3. 앙상블(Ensemble)

- 여러 모델을 함께 사용

    (의사결정나무, KNN, LDA, 로지스틱)

    예측 알고리즘 조합 →예측 성능 향상

    Random Forest, Boosted Tree

    좋은 의사결정나무를 모아서 숲을 만든다.

- 설명보다 예측이 중요한 경우 사용

## Random Forest

좋은 의사결정나무를 모아서 숲을 만든다 

- 장점

    중요한 변수 판별 가능: *before, after 값으로 판단*

- 단점

    숲이 되면서 해석 가능한 모델의 장점 사라짐 

### 1. Bootstrap 사용

데이터로부터 복원추출 

### 2. Forest 생성

무작위로 예측 변수를 선택 → 모델 구축

*의사결정나무: 기준 지표 사용 vs Random Forest: 무작위*

### 3. 결과

분류 → voting

예측 → 평균