# 군집화

# 1. 정의

- 군집화: **유사한 속성**을 갖는 데이터를 묶어 전체 데이터를 **군집으로 나누는 것**

    사전의 정의된 범주 X = 비지도 학습 

- 좋은 군집화의 기준
    1. 동일한 군집: 유사도↑
    2. 다른 군집: 유사도 ↓

    군집 내 거리 최소화 + 군집 간 거리 최대화

# 2. 유사도 척도

## 1. 유클리디언 거리

![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled.png)

## 2. 맨하탄 거리

![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%201.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%201.png)

## 3. 마할노비스 거리

![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%202.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%202.png)

- 데이터의 특성(변수 내 분산, 공분산)을 반영
- covariance matrix = identity matrix (원) → 유클리디언 거리와 동일

# 3. 군집화 알고리즘

## 1. 계층적 군집화(Hierachical Clustering)

- 계층적 트리 모델 이용
- **가까운 집단부터 차근차근 묶어 나가는 방식**

    모든 데이터 사이의 거리에 대한 유사도 행렬 계산 → 거리가 인접한 데이터끼리 군집 형성 → 유사도 행렬 갱신

    <군집과 군집의 거리 계산법>

    1. 가장 작은 값
    2. 가장 큰 값
    3. 평균값
    4. 평균과 평균 사이의 거리
    5. Ward 거리

        ![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%203.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%203.png)

        1. 모든 데이터(서로 다른 군집 모두 포함) 의 중심
        2. 구한 중심 ~ 모든 데이터 사이의 거리
        3. 각 군집의 중심
        4. 각 군집의 중심 ~ 군집 내 데이터 
        - 최종 결과 ↑ → 유사도 ↓, 최종결과 ↓ → 유사도 ↑
- 덴드로그램으로 시각화

    *덴드로그램: 개체들이 결합되는 순서를 나타내는 트리형태 구조*

- 사전에 군집의 수를 정하지 않아도 수행 가능 ↔ Kmeans

    덴드로그램 생성 후 적절한 수준에서 자름 

## 2. 분리형 군집화

- 특정 기준에 의해 동시 구분
- 모든 개체들은 군집 중 하나에 속함

### K 평균 군집화 (K-means Clustering)

- K = 사전의 정해진 군집 수

    <K값 선정 방법>

    성능 평가 지표를 통해 최적의 군집 수 (K) 선택

    - 성능 평가 지표
        1. 내부 평가 지표
            1. Dunn Index
            2. Sum of Square(SSE)

                ![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%204.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%204.png)

                군집 내 최소화 가능, 군집 간 거리 최대화 불가능 

        2. 외부 평가 지표: Rand index, Jaccard Coefficient
        3. silhouette 통계량

            ![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%205.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%205.png)

            - a(i) = i번째 데이터와 **같은 군집** 내에 있는 모든 데이터 사이의 평균 거리

                a(i) ↓ → Good

            - b(i) = i번째 데이터와 **다른 군집** 내에 있는 모든 데이터 사이의 최소 거리

                b(i) ↑ → Good

            - S값이 1에 가까울 수록  Good, -1에 가까울 수록 Bad

                S값이 0.5보다 크면 일반적으로 타당

            - **주로 2번째로 1에 가까운 K값을 선정**

                ![%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%206.png](%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%AA%207fb09311422e40a0bc4ba860721cffe6/Untitled%206.png)

                두 번째로 1에 가까운 값인 0.650이 더 잘 분류

- 군집: 하나의 중심을 가짐
- 개체: 가까운 중심에 할당

K개의 **중심을 임의로** 생성 → 생성된 중심을 기준으로 군집 할당 (가장 가까운 중심에 할당) → 각 군집의 중심 다시 계산: 각 군집의 평균 → 중심이 변하지 않을 때까지 반복

- 단점
    1. 초기 중심 설정(랜덤 초기화)이 최종 결과에 영향을 미칠 수 있음

        <랜덤 초기화 단점 극복 방법>

        1. 가장 여러번 나타나는 군집을 사용

            앙상블 결과 통합

        2. 데이터 분포 정보를 활용한 초기화 선정

            대부분의 경우 데이터의 분포를 알 수 없으므로 잘 사용되지 않는 방법

        3. 샘플링 데이터를 활용하여 계층적 군집화 → 초기 군집 중심으로 사용

            자주 사용되는 방법

        ⇒ 많은 경우 초기 중심이 최종 결과에 영향을 미치지 않는다

    2. 서로 다른 크기 / 서로 다른 밀도 / 지역적 패턴이 존재하는 군집을 잘 찾지 못함

        지역적 패턴 = 특이한 구조: 중심을 기준으로 군집을 할당하기 때문에 구조가 특이하면 Kmeans를 사용할 수 없다.

## 3. 분포 기반 군집화

- 높은 밀도를 갖는 세부영역들로 구분
- 모든 개체들이 반드시 포함되지 않음 → 잡음 처리

### DBSCAN(Density Based Clustering)

- 높은 밀도 → Group, 낮은 밀도 → 이상치 or 잡음
    1. 핵심 자료 (core point)

        ε-neighborhood가 M개 이상의 데이터 포함

    2. 주변 자료 (border point)

        핵심자료 X, ε-neighborhood에 핵심자료 포함 

    3. 잡음 자료 (noise point)

        핵심자료 X, 주변자료 X

- 파라미터: ε, M

    ε: 군집에 이웃을 포함할 수 있는 최대 거리 

     너무 작으면 → 많은 데이터가 잡음, 너무 크면 → 군집의 개수 적음

    HDBSCAN으로 자동 설정

    [](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)

    ```python
    HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,
        gen_min_span_tree=True, leaf_size=40, memory=Memory(cachedir=None),
        metric='euclidean', min_cluster_size=5, min_samples=None, p=None)
    ```

    [Clustering: HDBSCAN](https://teamdable.github.io/techblog/Unsupervised-hdbscan)

    HDBSCAN에 대한 설명글

    M: 군집을 형성하기 위해 필요한 최소 이웃 수 

    특성 변수 개수 + 1

- DBSCAN 과정
    1. 임의 데이터에 군집1 부여
    2. 임의 데이터의 ε-NN 구함
        1. 데이터 수 < M → 잡음자료
        2. 데이터 수 > M → ε-NN 모두 군집1 부여
        3. 군집 1 모든 데이터의 ε-NN의 크기 < M 이 될 때까지 반복
    3. 군집 2에 대해 반복

        모든 데이터에 군집이 할당되거나 잡음으로 분류될 때까지 반복